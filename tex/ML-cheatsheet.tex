\documentclass[10pt,letterpaper]{article}
\usepackage{multicol}
\usepackage{calc}
\usepackage{ifthen}
\usepackage[landscape]{geometry}
\usepackage{hyperref}
\usepackage{amsmath}

% To make this come out properly in landscape mode, do one of the following
% 1.
%  pdflatex latexsheet.tex
%
% 2.
%  latex latexsheet.tex
%  dvips -P pdf  -t landscape latexsheet.dvi
%  ps2pdf latexsheet.ps


% If you're reading this, be prepared for confusion.  Making this was
% a learning experience for me, and it shows.  Much of the placement
% was hacked in; if you make it better, let me know...


% 2008-04
% Changed page margin code to use the geometry package. Also added code for
% conditional page margins, depending on paper size. Thanks to Uwe Ziegenhagen
% for the suggestions.

% 2006-08
% Made changes based on suggestions from Gene Cooperman. <gene at ccs.neu.edu>


% To Do:
% \listoffigures \listoftables
% \setcounter{secnumdepth}{0}


% This sets page margins to .5 inch if using letter paper, and to 1cm
% if using A4 paper. (This probably isn't strictly necessary.)
% If using another size paper, use default 1cm margins.
\ifthenelse{\lengthtest { \paperwidth = 11in}}
	{ \geometry{top=.5in,left=.5in,right=.5in,bottom=.5in} }
	{\ifthenelse{ \lengthtest{ \paperwidth = 297mm}}
		{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
		{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
	}

% Turn off header and footer
\pagestyle{empty}
 

% Redefine section commands to use less space
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%x
                                {\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
                                {-1explus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%
                                {\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {1ex plus .2ex}%
                                {\normalfont\small\bfseries}}
\makeatother

% Define BibTeX command
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% Don't print section numbers
\setcounter{secnumdepth}{0}


\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}


% -----------------------------------------------------------------------

\begin{document}

\raggedright
\footnotesize
\begin{multicols}{3}


% multicol parameters
% These lengths are set only within the two main columns
%\setlength{\columnseprule}{0.25pt}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}

\begin{center}
     \Large{\textbf{Machine Learning Cheat Sheet}} \\
\end{center}

\section{Estimating Parameters}

\subsection{Likelihood of $\theta$ given the sample D}

\begin{equation*}
l(\theta | D) = p(D | \theta) = \prod_t p(x^t | \theta)
\end{equation*}

\subsection{Log likelihood}
\begin{equation*}
L(\theta | D) = \log l(\theta | D) = \sum_t \log p(x^t | \theta)
\end{equation*}

\subsection{Maximum Likelihood Estimation (MLE)}

\begin{equation*}
	\theta_{MLE} = \underset{\theta}{\argmax} \ L(\theta | D)
\end{equation*}

\subsection{Bernoulli}
\begin{equation*}
P(x) = p_0^x(1-p_0)^{(1-x)}
\end{equation*}
\begin{equation*}
L(p_0 | D) = \log \prod_t p_0^{x^t}(1-p_0)^{(1-x^t)}
\end{equation*}
\begin{equation*}
MLE: p_0 = \sum_t x^t / N
\end{equation*}
\begin{align*}
\theta_{MLE} &= \argmax_{\theta} L(p_0 | D) \\ 
&= \argmax_{\theta} \log \prod_t p_0^{x^t}(1-p_0)^{(1-x^t)} \\
&= \argmax_{\theta} \sum_t \log p_0^{x^t}(1-p_0)^{(1-x^t)} \\
&= \argmax_{\theta} \sum_t [x^t \log p_0 + (1 - x^t) \log(1 - p_0)]
\end{align*}
\begin{align*}
\frac{\partial L(p_0|D)}{\partial p_0} &= 0 \\
\frac{\partial L(p_0|D)}{\partial p_0} &=
\frac{\sum_t x^t}{p_0} + \frac{\sum_t (1 - x^t)}{(1 - p_0)} = 0 \\
(\sum_{i=1}^{n} x_i) \cdot (1 - \theta) &= \theta \cdot \sum_{i=1}^{n} (1-x_i) \\ 
\sum_{t} x^t - \theta \cdot \sum_{t} &= \theta \cdot \sum_{t} (1-x^t) \\ 
\theta = \frac{\sum_{t} x^t)}{\sum_{t} x^t + \sum_{t} (1-x^t)} &= \frac{n_1}{n}
\end{align*}

\subsection{Gaussian (Normal)}

\begin{align*}
p(\theta) &= \frac{1}{\sqrt{2 \pi} \sigma_1} e ^{-\frac{(\theta - \mu_1)^2}{2 \sigma_1^2}} \\
p(x | \theta) &= \frac{1}{\sqrt{2 \pi} \sigma_2} e ^{-\frac{(x - \theta)^2}{2 \sigma_2^2}} \\
\theta_{MLE} &= \argmax_{\theta} p(\theta | D ) = \argmax_{\theta} p(D |\theta) p(\theta) \\
&= \argmax_{\theta} \log(p(D |\theta) p(\theta)) \\
&= \argmax_{\theta} \log p(D |\theta) + \log p(\theta) \\
\end{align*}
\begin{align*}
&= \argmax (- \frac{1}{2} \log 2 \pi \sigma_2^2 - \frac{(x-\theta)^2}{2\sigma_2^2} - \frac{1}{2}\log 2 \pi \sigma_1^2 - \frac{(\theta - \mu_1)^2}{2 \sigma_1^2} ) \\
&= \argmax (-\frac{(x-\theta)^2}{2\sigma_2^2} - \frac{(\theta - \mu_1)^2}{2 \sigma_1^2})
\end{align*}
$\frac{\partial}{\partial \theta} (-\frac{(x-\theta)^2}{2\sigma_2^2} - \frac{(\theta - \mu_1)^2}{2 \sigma_1^2}) = 0$ \\
$\frac{x-\theta}{\sigma_2^2} - \frac{\theta - \mu_1}{ \sigma_1^2} = \sigma_1^2 (x-\theta) - \sigma_2^2(\theta - \mu_1) = 0$\\
$\theta = \frac{\sigma_1^2}{\sigma_1^2 + \sigma_2^2} x + \frac{\sigma_2^2}{\sigma_1^2 + \sigma_2^2} \mu_1$  

\subsection{Maximum a Posteriori (MAP)}

\begin{align*}
	\underset{\theta}{\argmax} \ P(\theta | D) &= \underset{\theta}{\argmax} \ \frac{P(D | \theta) P(\theta)}{P(D)} \\
	&= \underset{\theta}{\argmax} \ P(D | \theta) P(\theta)
\end{align*}

\section{Linear discriminant analysis}

\subsection{Two Classes}

\begin{align*}
g(x) &= g_1(x) - g_2(x) = (w_1^T x +w_{10}) - (w_2^T x +w_{20}) \\
&= (w_1 - w_2)^T x + (w_{10} - w_{20}) = w^T x + w_0
\end{align*}\\

$choose \begin{cases}
C_1 \quad \text{if }g(x) > 0 \\
C_2 \quad \text{otherwise}
\end{cases}$

\subsection{Logistic discrimination}

$\log \frac{p(x|C_1)}{p(x|C_2)} = w^Tx + w_0$

\section{Regression}

\subsection{Linear regression}

Find optimal $w$ and $b$ such that the average distance of points from the line is minimized:
\begin{equation*}
\underset{w,b}{\argmax} \ \sum_{i=1}^{n} \frac{1}{2} \left( w x_i + b - y_i \right)^2
\end{equation*}
Differentiate, and solution is
\begin{equation*}
\left[ \sum_{i = 1}^{n} x_i x_i^T \right] w = \sum_{i = 1}^{n} y_i x_i
\end{equation*}

$y$ as Gaussian:
\begin{align*}
y_i &= \theta^T x_i + \epsilon\\
\epsilon &\sim N(0, \gamma^2)\\
P(\epsilon_i) &= \frac{1}{\sqrt{2\pi} \sigma} exp \left( - \frac{\epsilon_i^2}{2 \sigma^2} \right)\\
P(y_i | x_i, \theta) &= \frac{1}{\sqrt{2 \pi} \sigma} exp \left( - \frac{(y_i - \theta^T x_i)^2}{2 \sigma^2} \right)
\end{align*}

Eigenvalues of $x^T x$ are positive. Use $x^T M x \geq 0 \:\forall x \in R^n$ to prove this:
\begin{align*}
v^T \sum_{i} x_i x_i^T v \geq 0\\
\sum_{i} (v^T x_i)^2 \geq 0
\end{align*}

\emph{Note} that when the data does not span the entire space, $x^T x$ does not have full rank, and thus is non-invertible. This can be fixed by either decomposition through removing eigenvalues below a certain threshold, or adding a small $\epsilon$ to the diagonal of the matrix.

\subsection{Mean Squared Error}
\begin{equation*}
Err(g, D) = \frac{1}{N} \sum_k [r^k - g(x^t)]^2 	
\end{equation*}

\subsection{Multiple linear regression}

$D =\begin{bmatrix}
    1 & x_{1}^{1} & x_{2}^{1} & \dots & x_{k}^{1} \\
    1 & x_{1}^{2} & x_{2}^{2} & \dots & x_{k}^{2} \\
    \dots & \dots & \dots &\dots & \dots \\
    1 & x_{1}^{N} & x_{2}^{N} & \dots & x_{k}^{N} \\
\end{bmatrix}$ \qquad
$r = 
\begin{bmatrix}
r^1 \\
r^2 \\
\dots \\
r^N     
\end{bmatrix}
$ \\
\text{ }\\

$w = (D^T D)^{-1} D^T r$

\begin{equation*}
g(x) = w_k x_k + w_{k−1} x_{k−1} + \cdots + w_1 x_1 + w_0
\end{equation*}

\section{Classification}

\subsection{Bayes' rule}
\begin{equation*}
P(C_i|D) = \frac{P(C_i) p(D|C_i)}{p(D)}
\rightarrow posterior = \frac{prior \cdot likelihood}{evidence}
\end{equation*}

$\sum_i P(C_i) = 1$\\
$p(D) = \sum_i P(C_i)p(D|C_i)$\\
$\sum_i p(C_i | D) = 1$\\

\subsection{Naive Bayes}
Assumes that features are independent.\\
\emph{Classifies} input vector $D = <x_1, ..., x_k>$ as class $C_i$ according to
\begin{align*}
	\underset{i}{\argmax} \ P(C_i | D)
	&= \underset{i}{\argmax} \ P(x_1, ..., x_n | C_i) P(C_i)\\
	&= \underset{i}{\argmax} \ P(C_i) \prod_{k}^{n} P(x_k | C_i)
\end{align*}

\begin{itemize}
	\item \emph{Discrete $x_k$} -- $P(x_k | C_i) = \frac{count(X_k = x_k, C_i)}{count(C_i)}$
	
	For smoothing $m$, use $P(x_k | C_i) = \frac{count(X_k = x_k, C_i) + m}{count(C_i) + N \cdot m}$, where $N$ is the number of different possible values for $X_k$
	\item \emph{Continuous $x_k$} -- Can use any PDF, but usually use Gaussian $P(x_k | C_i) = \mathcal{N}(\mu, \sigma^2) = \frac{1}{\sqrt{2 \pi}\sigma} e^{-\frac{(x_k-\mu)^2}{2\sigma^2}} $, where $\mu$ and $\sigma$ are, respectively, the average and variance. The Gaussian distribution already provides smoothing.
\end{itemize}

\subsection{Losses and Risks}

\begin{tabular}{ | l | l l l | } 
\hline
& \multicolumn{3}{|c|}{Class} \\
Action & $C_1$ & $\cdots$ & $C_k$  \\
\hline
$\alpha_1$ & $\lambda_{11}$ & $\cdots$ &  $\lambda_{1k}$  \\
$\cdots$ & $\cdots$ & $\cdots$ & $\cdots$\\
$\alpha_i$ & $\lambda_{i1}$ & $\cdots$ & $\lambda_{ik}$  \\
\hline
\end{tabular}

\begin{equation*}
R(\alpha_i | D) = \sum_k \lambda_{ik} P(C_k | D)
\end{equation*}
Choose $\alpha_i$ if $R(\alpha_i | D) = \argmin_k R(\alpha_k|D)$\\


\section{Appendix}
\subsection{Transpose of a matrix}
Switch the row and column indices of the matrix.

$A =\begin{bmatrix}
    a_{1,1} & a_{1,2} & \dots & a_{1,j} \\
    a_{2,1} & a_{2,2} & \dots & a_{2,j} \\
    \dots & \dots & \dots  &\dots        \\
    a_{i,1} & a_{i,2} & \dots & a_{i,j} \\
\end{bmatrix}$  $A^T =\begin{bmatrix}
    a_{1,1} & a_{2,1} & \dots & a_{i,1} \\
    a_{1,2} & a_{2,2} & \dots & a_{i, 2} \\
    \dots & \dots & \dots  &\dots        \\
    a_{1,j} & a_{2, j} & \dots & a_{i, j} \\
\end{bmatrix}$

\subsection{Inverse of a matrix}
$\begin{bmatrix}
    a_{1,1} & a_{1,2} \\
    a_{2,1} & a_{2,2} \\
\end{bmatrix} ^{-1} = 
\frac{1}{a_{1,1} a_{2,2} - a_{1,2} a_{2,1}}
\begin{bmatrix}
	a_{2,2} & -a_{1,2} \\
    -a_{2,1} & a_{1,1} \\
\end{bmatrix}
$ \\
Generic matrix\\
$A = \begin{bmatrix}
    a_{1,1} & a_{1,2} & a_{1,3} \\
    a_{2,1} & a_{2,2} & a_{2,3} \\
    a_{3,1} & a_{3,2} & a_{3,3} \\
\end{bmatrix}$ \\ 

Calculate the "Matrix of Minors".\\
$A' = \begin{bmatrix}
    a_{1,1}' & a_{1,2}' & a_{1,3}' \\
    a_{2,1}' & a_{2,2}' & a_{2,3}' \\
    a_{3,1}' & a_{3,2}' & a_{3,3}' \\
\end{bmatrix}$\\

$a_{1,1}' = \begin{bmatrix}
    \bullet &  &  \\
    	 & a_{2,2} & a_{2,3} \\
    	 & a_{3,2} & a_{3,3} \\
\end{bmatrix} = a_{2,2} a_{3,3} - a_{2,3}a_{3,2}$

Apply a "checkerboard" of minuses to the "Matrix of Minors" and obtain the "Matrix of Cofactors". \\
$A' \rightarrow \begin{bmatrix}
    + & - & + \\
    - & + & - \\
    + & - & + \\
\end{bmatrix} \rightarrow A''$

Transpose the "Matrix of Cofactors"(Adjugate)

Multiply by 1/Determinant of the original matrix
$A^{-1} = \frac{1}{det(A)} (A'')^T$
\smallskip
\rule{0.3\linewidth}{0.25pt}
\scriptsize

Copyright \copyright\ 2018 Antonio Mallia

\href{https://www.antoniomallia.it}{https://www.antoniomallia.it}

\end{multicols}
\end{document}
